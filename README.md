# Vision-Language Model Adversarial Evaluation Project

This project evaluates the outputs of adversarial attacks on Vision-Language Models (VLMs) to detect harmful content (e.g., nudity or violence). The evaluation process uses multiple stages, including structured query evaluation, rule patterns, and OpenAI's API, with ambiguous cases flagged for human annotation.

## Project Structure
The project is organized into the following directories and files:

```
VLM_Jailbreak_Evaluation/
├── data/                   # Output data to run analysis on
├── notebooks/              # Jupyter notebooks for analysis
├── scripts/                # Python scripts for analysis
├── results/                # Evaluation results
├── README.md               # Project documentation
└── requirements.txt        # List of dependencies
```

## Requirements

Install the required packages with:
```sh
pip install -r requirements.txt
```
